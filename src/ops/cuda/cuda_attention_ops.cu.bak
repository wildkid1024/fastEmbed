#include "ops/cuda/cuda_attention_ops.h"
#include "ops/cuda/cuda_matrix_ops.h"
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <stdexcept>
#include <vector>
#include <cublas_v2.h>
#include <device_launch_parameters.h>


CUDAAttentionOps::CUDAAttentionOps() : cuda_matrix_ops(std::make_unique<CUDAMatrixOps>()) {}


const int BLOCK_SIZE = 32; // 定义块大小，适合大多数GPU架构

__global__ void multiHeadAttentionKernel(
    const float* input, const float* weight_q, const float* weight_k, const float* weight_v,
    const float* query_bias, const float* key_bias, const float* value_bias,
    float* output, int batch_size, int seq_len, int embed_dim, int num_heads, int head_dim) {
    // 计算当前头索引和批次内索引
    const int head_idx = blockIdx.z;
    const int batch_idx = blockIdx.y;
    const int seq_pos = blockIdx.x * blockDim.x + threadIdx.x;

    if (seq_pos >= seq_len || head_idx >= num_heads) return;

    // 头偏移量计算
    const int head_offset = head_idx * head_dim;
    const int batch_offset = batch_idx * seq_len * embed_dim;

    // 声明共享内存用于权重和局部计算
    __shared__ float s_wq[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float s_wk[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float s_wv[BLOCK_SIZE][BLOCK_SIZE];

    // 局部寄存器存储QKV计算结果
    float q[BLOCK_SIZE] = {0.0f};
    float k[BLOCK_SIZE] = {0.0f};
    float v[BLOCK_SIZE] = {0.0f};

    // 分块加载权重矩阵并计算QKV
    // 修正1: 修复矩阵乘法维度 (输入×权重转置)
    for (int b = 0; b < embed_dim; b += BLOCK_SIZE) {
        // 加载权重块(转置布局)
        const int w_idx = head_offset * embed_dim + threadIdx.y * embed_dim + b + threadIdx.x;
        s_wq[threadIdx.x][threadIdx.y] = weight_q[w_idx];
        s_wk[threadIdx.x][threadIdx.y] = weight_k[w_idx];
        s_wv[threadIdx.x][threadIdx.y] = weight_v[w_idx];
        __syncthreads();

        // 矩阵乘法 (input[seq_pos] × weight^T)
        for (int i = 0; i < BLOCK_SIZE; ++i) {
             const float x = input[batch_offset + seq_pos * embed_dim + b + i];
            q[threadIdx.x] += x * s_wq[i][threadIdx.y];
            k[threadIdx.x] += x * s_wk[i][threadIdx.y];
            v[threadIdx.x] += x * s_wv[i][threadIdx.y];
        }
        __syncthreads();
    }

    // 添加偏置
    for (int i = 0; i < head_dim; ++i) {
        q[i] += query_bias[head_offset + i];
        k[i] += key_bias[head_offset + i];
        v[i] += value_bias[head_offset + i];
    }
    // 添加偏置后打印QKV值
    if (seq_pos == 0 && head_idx == 0) { // 仅在首个序列位置和头打印
        printf("Head %d, Seq %d - QKV values after bias:\n", head_idx, seq_pos);
        printf("Q: ");
        for (int i = 0; i < head_dim; ++i) printf("%.3f ", q[i]);
        printf("\nK: ");
        for (int i = 0; i < head_dim; ++i) printf("%.3f ", k[i]);
        printf("\nV: ");
        for (int i = 0; i < head_dim; ++i) printf("%.3f ", v[i]);
        printf("\n");
    }
    __syncthreads();

    // 计算注意力分数（简化版，实际实现需考虑分块和数值稳定性）
    __shared__ float s_scores[BLOCK_SIZE][BLOCK_SIZE];
    float score = 0.0f;
    for (int i = 0; i < head_dim; ++i) {
        score += q[i] * k[i];
    }
    score /= sqrtf(static_cast<float>(head_dim));
    s_scores[threadIdx.y][threadIdx.x] = score;
    __syncthreads();

    // Softmax和输出计算（实际实现需使用Warp级归约）
    float attn_weight = s_scores[threadIdx.y][threadIdx.x];
    float output_val = 0.0f;
    for (int i = 0; i < head_dim; ++i) {
        output_val += attn_weight * v[i];
    }

    // 写入输出
    // 修正2: 数值稳定的Softmax实现
    float scores[BLOCK_SIZE];
    float max_score = -1e20;
    for (int i = 0; i < seq_len; ++i) {
        scores[i] = q[threadIdx.x] * k[i];  // 修正QK点积计算
        max_score = max(max_score, scores[i]);
    }

    // 计算指数和与softmax
    float exp_sum = 0.0f;
    for (int i = 0; i < seq_len; ++i) {
        scores[i] = expf(scores[i] - max_score);  // 数值偏移
        exp_sum += scores[i];
    }

    // 修正3: 正确计算注意力输出
    output_val = 0.0f;
    for (int i = 0; i < seq_len; ++i) {
        output_val += (scores[i] / exp_sum) * v[i];
    }

    // 修正4: 确保输出索引正确
    const int output_idx = batch_offset + seq_pos * embed_dim + head_offset + threadIdx.x;
    if (output_idx < batch_size * seq_len * embed_dim) {
        output[output_idx] = output_val;
    }
    __syncthreads();
}

// 主机端函数
std::vector<float> CUDAAttentionOps::multi_head_attention(
    const std::vector<float>& input, const std::vector<float>& weight_q, const std::vector<float>& weight_k,
    const std::vector<float>& weight_v, const std::vector<float>& query_bias, const std::vector<float>& key_bias,
    const std::vector<float>& value_bias, size_t num_heads, size_t embedding_dim) {
    const size_t batch_size = 1;
    const size_t seq_len = input.size() / (embedding_dim * batch_size);
    const size_t head_dim = embedding_dim / num_heads;
    const size_t output_size = input.size();

    // 设备内存分配
    float *d_input, *d_output, *d_wq, *d_wk, *d_wv, *d_qb, *d_kb, *d_vb;
    cudaMalloc(&d_input, input.size() * sizeof(float));
    cudaMalloc(&d_output, output_size * sizeof(float));
    cudaMalloc(&d_wq, weight_q.size() * sizeof(float));
    cudaMalloc(&d_wk, weight_k.size() * sizeof(float));
    cudaMalloc(&d_wv, weight_v.size() * sizeof(float));
    cudaMalloc(&d_qb, query_bias.size() * sizeof(float));
    cudaMalloc(&d_kb, key_bias.size() * sizeof(float));
    cudaMalloc(&d_vb, value_bias.size() * sizeof(float));

    // 数据拷贝到设备
    cudaMemcpy(d_input, input.data(), input.size() * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_wq, weight_q.data(), weight_q.size() * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_wk, weight_k.data(), weight_k.size() * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_wv, weight_v.data(), weight_v.size() * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_qb, query_bias.data(), query_bias.size() * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_kb, key_bias.data(), key_bias.size() * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_vb, value_bias.data(), value_bias.size() * sizeof(float), cudaMemcpyHostToDevice);

    // 核函数配置
    const dim3 block(BLOCK_SIZE, BLOCK_SIZE);
    const dim3 grid((seq_len + block.x - 1) / block.x, batch_size, num_heads);

    // 启动核函数
    multiHeadAttentionKernel<<<grid, block>>>(
        d_input, d_wq, d_wk, d_wv, d_qb, d_kb, d_vb,
        d_output, batch_size, seq_len, embedding_dim, num_heads, head_dim);

    // 拷贝结果回主机
    std::vector<float> output(output_size);
    cudaMemcpy(output.data(), d_output, output_size * sizeof(float), cudaMemcpyDeviceToHost);

    // 释放设备内存
    cudaFree(d_input); cudaFree(d_output); cudaFree(d_wq); cudaFree(d_wk);
    cudaFree(d_wv); cudaFree(d_qb); cudaFree(d_kb); cudaFree(d_vb);

    return output;
}